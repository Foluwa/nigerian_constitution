{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nigerian_Constitution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf76FCwxMYby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "179df490-932f-4158-a618-4f5504bd86e6",
        "tags": []
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "# Incase punkt is not found uncomment this line and rerun\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYx0b8dpNPnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizer\n",
        "\n",
        "def tokenize(my_sentence):\n",
        "    return nltk.word_tokenize(my_sentence)\n",
        "\n",
        "# Stemmer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer= PorterStemmer()\n",
        "\n",
        "def stemming(single_word):\n",
        "    return stemmer.stem(single_word.lower())\n",
        "\n",
        "def bag_of_words_converter(tokenized_stemmed_sentence,bag_of_words_original):\n",
        "    \n",
        "    vector=np.zeros(len(bag_of_words_original),dtype=np.float32)\n",
        "    for word in tokenized_stemmed_sentence:\n",
        "        if word in bag_of_words_original:\n",
        "            my_index=bag_of_words_original.index(word)\n",
        "            vector[my_index]=1\n",
        "    return vector"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzBMdDjSNS8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load intents\n",
        "with open('../data/constitution_intents.json','r') as f:\n",
        "    intents=json.load(f)\n",
        "\n",
        "# print(intent)\n",
        "\n",
        "text_tag_tuple=[]\n",
        "tags=[]\n",
        "all_words_array=[]\n",
        "for intent in intents[\"intents\"]:\n",
        "    tag=intent[\"tag\"]\n",
        "    tags.append(tag)\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        tokenized_sentence=tokenize(pattern)\n",
        "        text_tag_tuple.append((tokenized_sentence,tag))\n",
        "        all_words_array.extend(tokenized_sentence)\n",
        "\n",
        "\n",
        "# now making sorted and unieqe set of all_words array\n",
        "other_characters=['?','!','.', ',']\n",
        "tags=sorted(set(tags))\n",
        "\n",
        "# Stemming each words and character removal\n",
        "all_words_array=[stemming(w) for w in all_words_array if w not in other_characters]\n",
        "all_words_array=sorted(set(all_words_array))\n",
        "\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLcmrZ03N6qC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f3c670d-5249-476b-e9d5-31473c0f8722",
        "tags": []
      },
      "source": [
        "my_sen=\"howsf aresafasf yousfx\"\n",
        "tok=tokenize(my_sen)\n",
        "print(tok)\n",
        "stem_version=[stemming(w) for w in tok]\n",
        "print(stem_version)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['howsf', 'aresafasf', 'yousfx']\n['howsf', 'aresafasf', 'yousfx']\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCEGdNawPVdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "x=glob.glob('../input/my-data1212/*')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZPI6s-JPYul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train=[]\n",
        "Y_train=[]\n",
        "\n",
        "for (sentence,tag) in text_tag_tuple:\n",
        "    sentence=[stemming(w) for w in sentence]\n",
        "    my_converted_vector=bag_of_words_converter(sentence,all_words_array)\n",
        "    index_of_label=tags.index(tag)\n",
        "    X_train.append(my_converted_vector)\n",
        "    Y_train.append(index_of_label)\n",
        "X_train=np.array(X_train)\n",
        "Y_train=np.array(Y_train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXgpQVXtRNDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pytorch network\n",
        "class chatbot_dataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.length_data=len(X_train)\n",
        "        self.x_data=X_train\n",
        "        self.y_data=Y_train\n",
        "    def __getitem__(self,idx):\n",
        "        return self.x_data[idx], self.y_data[idx]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(X_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maulY065RSfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_created=chatbot_dataset()\n",
        "train_loader=DataLoader(dataset_created,batch_size=8,shuffle=True,num_workers=2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWnLty7wRXih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now lets make the neural network\n",
        "\n",
        "class my_network(nn.Module):\n",
        "    def __init__(self,input_size,out_classes,hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1=nn.Linear(in_features=input_size,out_features=hidden_size)\n",
        "        self.linear2=nn.Linear(in_features=hidden_size,out_features=hidden_size)\n",
        "        self.linear3=nn.Linear(in_features=hidden_size,out_features=out_classes)\n",
        "        self.relu=nn.ReLU()\n",
        "        \n",
        "    def forward(self,t):\n",
        "        t=t\n",
        "        t=self.linear1(t)\n",
        "        t=self.relu(t)\n",
        "        t=self.linear2(t)\n",
        "        t=self.relu(t)\n",
        "        t=self.linear3(t)\n",
        "\n",
        "        return t"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQTnD8XYRZZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#now defining some basic variables\n",
        "input_size=len(all_words_array)\n",
        "output_size=len(tags)\n",
        "hidden_size=16\n",
        "#defining loss, optimizer etc\n",
        "#Now making arguments for above ftn\n",
        "\n",
        "\n",
        "my_model=my_network(input_size,output_size,hidden_size)\n",
        "learning_rate1=0.003\n",
        "my_loss=nn.CrossEntropyLoss()\n",
        "my_optimizer=torch.optim.Adam(my_model.parameters(),lr=learning_rate1)\n",
        "dynamic_learning_rate=torch.optim.lr_scheduler.StepLR(my_optimizer,step_size=7,gamma=0.1)\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#now time to make the model \n",
        "def my_train_model(model,data,optimizer,given_loss,scheduler,total_epochs=1000):\n",
        "\n",
        "    train_loss , train_acc, val_loss, val_accuracy = [],[],[],[]\n",
        "\n",
        "    my_sizes={ 'train': len(X_train)}\n",
        "    #first loop for the epochs\n",
        "    for i in range (total_epochs):\n",
        "            total_correct=0\n",
        "            for batch in data:\n",
        "                \n",
        "                \n",
        "                #now performing the forward steps \n",
        "                input_data,labels=batch\n",
        "                #put data into GPU processing if available\n",
        "                input_data=input_data.to(device)\n",
        "                labels=labels.to(device)\n",
        "                my_prediction=model(input_data)\n",
        "                #find loss\n",
        "                loss=given_loss(my_prediction,labels)\n",
        "                total_correct+=my_prediction.argmax(dim=1).eq(labels).sum().item()\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "#             if (i+1 % 100 == 0):\n",
        "            print(f'epoch {i+1}/1000,loss={loss.item():.4f}')\n",
        "            print(' Accuracy= ' +  str(total_correct/my_sizes[\"train\"]))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8jKLo5pReTp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "123a7ceb-7a44-48ba-f601-e8326a4ad785",
        "tags": [
          "outputPrepend"
        ]
      },
      "source": [
        "trained_model=my_train_model(model=my_model.to(device),data=train_loader,\n",
        "               optimizer=my_optimizer,\n",
        "                    given_loss=my_loss,\n",
        "               scheduler=dynamic_learning_rate)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "y= 1.0\nepoch 525/1000,loss=0.0002\n Accuracy= 1.0\nepoch 526/1000,loss=0.0003\n Accuracy= 1.0\nepoch 527/1000,loss=0.0003\n Accuracy= 1.0\nepoch 528/1000,loss=0.0003\n Accuracy= 1.0\nepoch 529/1000,loss=0.0001\n Accuracy= 1.0\nepoch 530/1000,loss=0.0002\n Accuracy= 1.0\nepoch 531/1000,loss=0.0003\n Accuracy= 1.0\nepoch 532/1000,loss=0.0002\n Accuracy= 1.0\nepoch 533/1000,loss=0.0002\n Accuracy= 1.0\nepoch 534/1000,loss=0.0003\n Accuracy= 1.0\nepoch 535/1000,loss=0.0001\n Accuracy= 1.0\nepoch 536/1000,loss=0.0003\n Accuracy= 1.0\nepoch 537/1000,loss=0.0003\n Accuracy= 1.0\nepoch 538/1000,loss=0.0001\n Accuracy= 1.0\nepoch 539/1000,loss=0.0003\n Accuracy= 1.0\nepoch 540/1000,loss=0.0002\n Accuracy= 1.0\nepoch 541/1000,loss=0.0002\n Accuracy= 1.0\nepoch 542/1000,loss=0.0003\n Accuracy= 1.0\nepoch 543/1000,loss=0.0004\n Accuracy= 1.0\nepoch 544/1000,loss=0.0002\n Accuracy= 1.0\nepoch 545/1000,loss=0.0002\n Accuracy= 1.0\nepoch 546/1000,loss=0.0001\n Accuracy= 1.0\nepoch 547/1000,loss=0.0002\n Accuracy= 1.0\nepoch 548/1000,loss=0.0001\n Accuracy= 1.0\nepoch 549/1000,loss=0.0004\n Accuracy= 1.0\nepoch 550/1000,loss=0.0002\n Accuracy= 1.0\nepoch 551/1000,loss=0.0001\n Accuracy= 1.0\nepoch 552/1000,loss=0.0004\n Accuracy= 1.0\nepoch 553/1000,loss=0.0001\n Accuracy= 1.0\nepoch 554/1000,loss=0.0003\n Accuracy= 1.0\nepoch 555/1000,loss=0.0002\n Accuracy= 1.0\nepoch 556/1000,loss=0.0003\n Accuracy= 1.0\nepoch 557/1000,loss=0.0002\n Accuracy= 1.0\nepoch 558/1000,loss=0.0003\n Accuracy= 1.0\nepoch 559/1000,loss=0.0001\n Accuracy= 1.0\nepoch 560/1000,loss=0.0004\n Accuracy= 1.0\nepoch 561/1000,loss=0.0004\n Accuracy= 1.0\nepoch 562/1000,loss=0.0003\n Accuracy= 1.0\nepoch 563/1000,loss=0.0001\n Accuracy= 1.0\nepoch 564/1000,loss=0.0004\n Accuracy= 1.0\nepoch 565/1000,loss=0.0002\n Accuracy= 1.0\nepoch 566/1000,loss=0.0004\n Accuracy= 1.0\nepoch 567/1000,loss=0.0002\n Accuracy= 1.0\nepoch 568/1000,loss=0.0004\n Accuracy= 1.0\nepoch 569/1000,loss=0.0004\n Accuracy= 1.0\nepoch 570/1000,loss=0.0001\n Accuracy= 1.0\nepoch 571/1000,loss=0.0002\n Accuracy= 1.0\nepoch 572/1000,loss=0.0001\n Accuracy= 1.0\nepoch 573/1000,loss=0.0002\n Accuracy= 1.0\nepoch 574/1000,loss=0.0001\n Accuracy= 1.0\nepoch 575/1000,loss=0.0001\n Accuracy= 1.0\nepoch 576/1000,loss=0.0003\n Accuracy= 1.0\nepoch 577/1000,loss=0.0002\n Accuracy= 1.0\nepoch 578/1000,loss=0.0002\n Accuracy= 1.0\nepoch 579/1000,loss=0.0003\n Accuracy= 1.0\nepoch 580/1000,loss=0.0002\n Accuracy= 1.0\nepoch 581/1000,loss=0.0002\n Accuracy= 1.0\nepoch 582/1000,loss=0.0001\n Accuracy= 1.0\nepoch 583/1000,loss=0.0003\n Accuracy= 1.0\nepoch 584/1000,loss=0.0002\n Accuracy= 1.0\nepoch 585/1000,loss=0.0002\n Accuracy= 1.0\nepoch 586/1000,loss=0.0003\n Accuracy= 1.0\nepoch 587/1000,loss=0.0004\n Accuracy= 1.0\nepoch 588/1000,loss=0.0001\n Accuracy= 1.0\nepoch 589/1000,loss=0.0002\n Accuracy= 1.0\nepoch 590/1000,loss=0.0001\n Accuracy= 1.0\nepoch 591/1000,loss=0.0001\n Accuracy= 1.0\nepoch 592/1000,loss=0.0002\n Accuracy= 1.0\nepoch 593/1000,loss=0.0003\n Accuracy= 1.0\nepoch 594/1000,loss=0.0002\n Accuracy= 1.0\nepoch 595/1000,loss=0.0003\n Accuracy= 1.0\nepoch 596/1000,loss=0.0002\n Accuracy= 1.0\nepoch 597/1000,loss=0.0002\n Accuracy= 1.0\nepoch 598/1000,loss=0.0003\n Accuracy= 1.0\nepoch 599/1000,loss=0.0002\n Accuracy= 1.0\nepoch 600/1000,loss=0.0003\n Accuracy= 1.0\nepoch 601/1000,loss=0.0001\n Accuracy= 1.0\nepoch 602/1000,loss=0.0002\n Accuracy= 1.0\nepoch 603/1000,loss=0.0003\n Accuracy= 1.0\nepoch 604/1000,loss=0.0001\n Accuracy= 1.0\nepoch 605/1000,loss=0.0002\n Accuracy= 1.0\nepoch 606/1000,loss=0.0002\n Accuracy= 1.0\nepoch 607/1000,loss=0.0002\n Accuracy= 1.0\nepoch 608/1000,loss=0.0001\n Accuracy= 1.0\nepoch 609/1000,loss=0.0002\n Accuracy= 1.0\nepoch 610/1000,loss=0.0002\n Accuracy= 1.0\nepoch 611/1000,loss=0.0003\n Accuracy= 1.0\nepoch 612/1000,loss=0.0002\n Accuracy= 1.0\nepoch 613/1000,loss=0.0002\n Accuracy= 1.0\nepoch 614/1000,loss=0.0001\n Accuracy= 1.0\nepoch 615/1000,loss=0.0001\n Accuracy= 1.0\nepoch 616/1000,loss=0.0001\n Accuracy= 1.0\nepoch 617/1000,loss=0.0001\n Accuracy= 1.0\nepoch 618/1000,loss=0.0001\n Accuracy= 1.0\nepoch 619/1000,loss=0.0002\n Accuracy= 1.0\nepoch 620/1000,loss=0.0003\n Accuracy= 1.0\nepoch 621/1000,loss=0.0001\n Accuracy= 1.0\nepoch 622/1000,loss=0.0001\n Accuracy= 1.0\nepoch 623/1000,loss=0.0002\n Accuracy= 1.0\nepoch 624/1000,loss=0.0001\n Accuracy= 1.0\nepoch 625/1000,loss=0.0003\n Accuracy= 1.0\nepoch 626/1000,loss=0.0001\n Accuracy= 1.0\nepoch 627/1000,loss=0.0001\n Accuracy= 1.0\nepoch 628/1000,loss=0.0002\n Accuracy= 1.0\nepoch 629/1000,loss=0.0001\n Accuracy= 1.0\nepoch 630/1000,loss=0.0002\n Accuracy= 1.0\nepoch 631/1000,loss=0.0002\n Accuracy= 1.0\nepoch 632/1000,loss=0.0002\n Accuracy= 1.0\nepoch 633/1000,loss=0.0002\n Accuracy= 1.0\nepoch 634/1000,loss=0.0003\n Accuracy= 1.0\nepoch 635/1000,loss=0.0001\n Accuracy= 1.0\nepoch 636/1000,loss=0.0001\n Accuracy= 1.0\nepoch 637/1000,loss=0.0002\n Accuracy= 1.0\nepoch 638/1000,loss=0.0001\n Accuracy= 1.0\nepoch 639/1000,loss=0.0001\n Accuracy= 1.0\nepoch 640/1000,loss=0.0002\n Accuracy= 1.0\nepoch 641/1000,loss=0.0002\n Accuracy= 1.0\nepoch 642/1000,loss=0.0000\n Accuracy= 1.0\nepoch 643/1000,loss=0.0003\n Accuracy= 1.0\nepoch 644/1000,loss=0.0001\n Accuracy= 1.0\nepoch 645/1000,loss=0.0002\n Accuracy= 1.0\nepoch 646/1000,loss=0.0001\n Accuracy= 1.0\nepoch 647/1000,loss=0.0002\n Accuracy= 1.0\nepoch 648/1000,loss=0.0002\n Accuracy= 1.0\nepoch 649/1000,loss=0.0001\n Accuracy= 1.0\nepoch 650/1000,loss=0.0001\n Accuracy= 1.0\nepoch 651/1000,loss=0.0001\n Accuracy= 1.0\nepoch 652/1000,loss=0.0001\n Accuracy= 1.0\nepoch 653/1000,loss=0.0001\n Accuracy= 1.0\nepoch 654/1000,loss=0.0002\n Accuracy= 1.0\nepoch 655/1000,loss=0.0002\n Accuracy= 1.0\nepoch 656/1000,loss=0.0001\n Accuracy= 1.0\nepoch 657/1000,loss=0.0001\n Accuracy= 1.0\nepoch 658/1000,loss=0.0001\n Accuracy= 1.0\nepoch 659/1000,loss=0.0002\n Accuracy= 1.0\nepoch 660/1000,loss=0.0000\n Accuracy= 1.0\nepoch 661/1000,loss=0.0002\n Accuracy= 1.0\nepoch 662/1000,loss=0.0002\n Accuracy= 1.0\nepoch 663/1000,loss=0.0002\n Accuracy= 1.0\nepoch 664/1000,loss=0.0001\n Accuracy= 1.0\nepoch 665/1000,loss=0.0003\n Accuracy= 1.0\nepoch 666/1000,loss=0.0001\n Accuracy= 1.0\nepoch 667/1000,loss=0.0001\n Accuracy= 1.0\nepoch 668/1000,loss=0.0001\n Accuracy= 1.0\nepoch 669/1000,loss=0.0001\n Accuracy= 1.0\nepoch 670/1000,loss=0.0001\n Accuracy= 1.0\nepoch 671/1000,loss=0.0001\n Accuracy= 1.0\nepoch 672/1000,loss=0.0002\n Accuracy= 1.0\nepoch 673/1000,loss=0.0001\n Accuracy= 1.0\nepoch 674/1000,loss=0.0002\n Accuracy= 1.0\nepoch 675/1000,loss=0.0001\n Accuracy= 1.0\nepoch 676/1000,loss=0.0000\n Accuracy= 1.0\nepoch 677/1000,loss=0.0002\n Accuracy= 1.0\nepoch 678/1000,loss=0.0001\n Accuracy= 1.0\nepoch 679/1000,loss=0.0000\n Accuracy= 1.0\nepoch 680/1000,loss=0.0002\n Accuracy= 1.0\nepoch 681/1000,loss=0.0001\n Accuracy= 1.0\nepoch 682/1000,loss=0.0002\n Accuracy= 1.0\nepoch 683/1000,loss=0.0001\n Accuracy= 1.0\nepoch 684/1000,loss=0.0002\n Accuracy= 1.0\nepoch 685/1000,loss=0.0001\n Accuracy= 1.0\nepoch 686/1000,loss=0.0001\n Accuracy= 1.0\nepoch 687/1000,loss=0.0001\n Accuracy= 1.0\nepoch 688/1000,loss=0.0002\n Accuracy= 1.0\nepoch 689/1000,loss=0.0001\n Accuracy= 1.0\nepoch 690/1000,loss=0.0001\n Accuracy= 1.0\nepoch 691/1000,loss=0.0002\n Accuracy= 1.0\nepoch 692/1000,loss=0.0001\n Accuracy= 1.0\nepoch 693/1000,loss=0.0002\n Accuracy= 1.0\nepoch 694/1000,loss=0.0001\n Accuracy= 1.0\nepoch 695/1000,loss=0.0001\n Accuracy= 1.0\nepoch 696/1000,loss=0.0001\n Accuracy= 1.0\nepoch 697/1000,loss=0.0000\n Accuracy= 1.0\nepoch 698/1000,loss=0.0002\n Accuracy= 1.0\nepoch 699/1000,loss=0.0000\n Accuracy= 1.0\nepoch 700/1000,loss=0.0002\n Accuracy= 1.0\nepoch 701/1000,loss=0.0002\n Accuracy= 1.0\nepoch 702/1000,loss=0.0001\n Accuracy= 1.0\nepoch 703/1000,loss=0.0002\n Accuracy= 1.0\nepoch 704/1000,loss=0.0001\n Accuracy= 1.0\nepoch 705/1000,loss=0.0002\n Accuracy= 1.0\nepoch 706/1000,loss=0.0001\n Accuracy= 1.0\nepoch 707/1000,loss=0.0001\n Accuracy= 1.0\nepoch 708/1000,loss=0.0002\n Accuracy= 1.0\nepoch 709/1000,loss=0.0001\n Accuracy= 1.0\nepoch 710/1000,loss=0.0001\n Accuracy= 1.0\nepoch 711/1000,loss=0.0000\n Accuracy= 1.0\nepoch 712/1000,loss=0.0001\n Accuracy= 1.0\nepoch 713/1000,loss=0.0002\n Accuracy= 1.0\nepoch 714/1000,loss=0.0001\n Accuracy= 1.0\nepoch 715/1000,loss=0.0001\n Accuracy= 1.0\nepoch 716/1000,loss=0.0002\n Accuracy= 1.0\nepoch 717/1000,loss=0.0001\n Accuracy= 1.0\nepoch 718/1000,loss=0.0001\n Accuracy= 1.0\nepoch 719/1000,loss=0.0000\n Accuracy= 1.0\nepoch 720/1000,loss=0.0001\n Accuracy= 1.0\nepoch 721/1000,loss=0.0000\n Accuracy= 1.0\nepoch 722/1000,loss=0.0002\n Accuracy= 1.0\nepoch 723/1000,loss=0.0000\n Accuracy= 1.0\nepoch 724/1000,loss=0.0000\n Accuracy= 1.0\nepoch 725/1000,loss=0.0000\n Accuracy= 1.0\nepoch 726/1000,loss=0.0002\n Accuracy= 1.0\nepoch 727/1000,loss=0.0002\n Accuracy= 1.0\nepoch 728/1000,loss=0.0001\n Accuracy= 1.0\nepoch 729/1000,loss=0.0002\n Accuracy= 1.0\nepoch 730/1000,loss=0.0001\n Accuracy= 1.0\nepoch 731/1000,loss=0.0001\n Accuracy= 1.0\nepoch 732/1000,loss=0.0001\n Accuracy= 1.0\nepoch 733/1000,loss=0.0001\n Accuracy= 1.0\nepoch 734/1000,loss=0.0001\n Accuracy= 1.0\nepoch 735/1000,loss=0.0000\n Accuracy= 1.0\nepoch 736/1000,loss=0.0001\n Accuracy= 1.0\nepoch 737/1000,loss=0.0000\n Accuracy= 1.0\nepoch 738/1000,loss=0.0001\n Accuracy= 1.0\nepoch 739/1000,loss=0.0000\n Accuracy= 1.0\nepoch 740/1000,loss=0.0001\n Accuracy= 1.0\nepoch 741/1000,loss=0.0001\n Accuracy= 1.0\nepoch 742/1000,loss=0.0002\n Accuracy= 1.0\nepoch 743/1000,loss=0.0002\n Accuracy= 1.0\nepoch 744/1000,loss=0.0000\n Accuracy= 1.0\nepoch 745/1000,loss=0.0001\n Accuracy= 1.0\nepoch 746/1000,loss=0.0001\n Accuracy= 1.0\nepoch 747/1000,loss=0.0001\n Accuracy= 1.0\nepoch 748/1000,loss=0.0001\n Accuracy= 1.0\nepoch 749/1000,loss=0.0001\n Accuracy= 1.0\nepoch 750/1000,loss=0.0000\n Accuracy= 1.0\nepoch 751/1000,loss=0.0001\n Accuracy= 1.0\nepoch 752/1000,loss=0.0002\n Accuracy= 1.0\nepoch 753/1000,loss=0.0001\n Accuracy= 1.0\nepoch 754/1000,loss=0.0000\n Accuracy= 1.0\nepoch 755/1000,loss=0.0001\n Accuracy= 1.0\nepoch 756/1000,loss=0.0000\n Accuracy= 1.0\nepoch 757/1000,loss=0.0001\n Accuracy= 1.0\nepoch 758/1000,loss=0.0001\n Accuracy= 1.0\nepoch 759/1000,loss=0.0001\n Accuracy= 1.0\nepoch 760/1000,loss=0.0001\n Accuracy= 1.0\nepoch 761/1000,loss=0.0001\n Accuracy= 1.0\nepoch 762/1000,loss=0.0000\n Accuracy= 1.0\nepoch 763/1000,loss=0.0001\n Accuracy= 1.0\nepoch 764/1000,loss=0.0000\n Accuracy= 1.0\nepoch 765/1000,loss=0.0000\n Accuracy= 1.0\nepoch 766/1000,loss=0.0002\n Accuracy= 1.0\nepoch 767/1000,loss=0.0001\n Accuracy= 1.0\nepoch 768/1000,loss=0.0001\n Accuracy= 1.0\nepoch 769/1000,loss=0.0000\n Accuracy= 1.0\nepoch 770/1000,loss=0.0001\n Accuracy= 1.0\nepoch 771/1000,loss=0.0002\n Accuracy= 1.0\nepoch 772/1000,loss=0.0001\n Accuracy= 1.0\nepoch 773/1000,loss=0.0001\n Accuracy= 1.0\nepoch 774/1000,loss=0.0001\n Accuracy= 1.0\nepoch 775/1000,loss=0.0000\n Accuracy= 1.0\nepoch 776/1000,loss=0.0000\n Accuracy= 1.0\nepoch 777/1000,loss=0.0001\n Accuracy= 1.0\nepoch 778/1000,loss=0.0001\n Accuracy= 1.0\nepoch 779/1000,loss=0.0001\n Accuracy= 1.0\nepoch 780/1000,loss=0.0001\n Accuracy= 1.0\nepoch 781/1000,loss=0.0001\n Accuracy= 1.0\nepoch 782/1000,loss=0.0001\n Accuracy= 1.0\nepoch 783/1000,loss=0.0001\n Accuracy= 1.0\nepoch 784/1000,loss=0.0001\n Accuracy= 1.0\nepoch 785/1000,loss=0.0001\n Accuracy= 1.0\nepoch 786/1000,loss=0.0002\n Accuracy= 1.0\nepoch 787/1000,loss=0.0000\n Accuracy= 1.0\nepoch 788/1000,loss=0.0000\n Accuracy= 1.0\nepoch 789/1000,loss=0.0001\n Accuracy= 1.0\nepoch 790/1000,loss=0.0002\n Accuracy= 1.0\nepoch 791/1000,loss=0.0000\n Accuracy= 1.0\nepoch 792/1000,loss=0.0000\n Accuracy= 1.0\nepoch 793/1000,loss=0.0002\n Accuracy= 1.0\nepoch 794/1000,loss=0.0001\n Accuracy= 1.0\nepoch 795/1000,loss=0.0001\n Accuracy= 1.0\nepoch 796/1000,loss=0.0002\n Accuracy= 1.0\nepoch 797/1000,loss=0.0001\n Accuracy= 1.0\nepoch 798/1000,loss=0.0001\n Accuracy= 1.0\nepoch 799/1000,loss=0.0002\n Accuracy= 1.0\nepoch 800/1000,loss=0.0001\n Accuracy= 1.0\nepoch 801/1000,loss=0.0002\n Accuracy= 1.0\nepoch 802/1000,loss=0.0001\n Accuracy= 1.0\nepoch 803/1000,loss=0.0001\n Accuracy= 1.0\nepoch 804/1000,loss=0.0001\n Accuracy= 1.0\nepoch 805/1000,loss=0.0000\n Accuracy= 1.0\nepoch 806/1000,loss=0.0000\n Accuracy= 1.0\nepoch 807/1000,loss=0.0000\n Accuracy= 1.0\nepoch 808/1000,loss=0.0000\n Accuracy= 1.0\nepoch 809/1000,loss=0.0001\n Accuracy= 1.0\nepoch 810/1000,loss=0.0001\n Accuracy= 1.0\nepoch 811/1000,loss=0.0000\n Accuracy= 1.0\nepoch 812/1000,loss=0.0001\n Accuracy= 1.0\nepoch 813/1000,loss=0.0002\n Accuracy= 1.0\nepoch 814/1000,loss=0.0001\n Accuracy= 1.0\nepoch 815/1000,loss=0.0001\n Accuracy= 1.0\nepoch 816/1000,loss=0.0001\n Accuracy= 1.0\nepoch 817/1000,loss=0.0001\n Accuracy= 1.0\nepoch 818/1000,loss=0.0001\n Accuracy= 1.0\nepoch 819/1000,loss=0.0001\n Accuracy= 1.0\nepoch 820/1000,loss=0.0001\n Accuracy= 1.0\nepoch 821/1000,loss=0.0001\n Accuracy= 1.0\nepoch 822/1000,loss=0.0001\n Accuracy= 1.0\nepoch 823/1000,loss=0.0000\n Accuracy= 1.0\nepoch 824/1000,loss=0.0001\n Accuracy= 1.0\nepoch 825/1000,loss=0.0001\n Accuracy= 1.0\nepoch 826/1000,loss=0.0000\n Accuracy= 1.0\nepoch 827/1000,loss=0.0001\n Accuracy= 1.0\nepoch 828/1000,loss=0.0000\n Accuracy= 1.0\nepoch 829/1000,loss=0.0001\n Accuracy= 1.0\nepoch 830/1000,loss=0.0000\n Accuracy= 1.0\nepoch 831/1000,loss=0.0001\n Accuracy= 1.0\nepoch 832/1000,loss=0.0001\n Accuracy= 1.0\nepoch 833/1000,loss=0.0001\n Accuracy= 1.0\nepoch 834/1000,loss=0.0001\n Accuracy= 1.0\nepoch 835/1000,loss=0.0001\n Accuracy= 1.0\nepoch 836/1000,loss=0.0001\n Accuracy= 1.0\nepoch 837/1000,loss=0.0001\n Accuracy= 1.0\nepoch 838/1000,loss=0.0001\n Accuracy= 1.0\nepoch 839/1000,loss=0.0000\n Accuracy= 1.0\nepoch 840/1000,loss=0.0001\n Accuracy= 1.0\nepoch 841/1000,loss=0.0000\n Accuracy= 1.0\nepoch 842/1000,loss=0.0001\n Accuracy= 1.0\nepoch 843/1000,loss=0.0001\n Accuracy= 1.0\nepoch 844/1000,loss=0.0001\n Accuracy= 1.0\nepoch 845/1000,loss=0.0001\n Accuracy= 1.0\nepoch 846/1000,loss=0.0001\n Accuracy= 1.0\nepoch 847/1000,loss=0.0000\n Accuracy= 1.0\nepoch 848/1000,loss=0.0000\n Accuracy= 1.0\nepoch 849/1000,loss=0.0002\n Accuracy= 1.0\nepoch 850/1000,loss=0.0001\n Accuracy= 1.0\nepoch 851/1000,loss=0.0001\n Accuracy= 1.0\nepoch 852/1000,loss=0.0001\n Accuracy= 1.0\nepoch 853/1000,loss=0.0001\n Accuracy= 1.0\nepoch 854/1000,loss=0.0001\n Accuracy= 1.0\nepoch 855/1000,loss=0.0001\n Accuracy= 1.0\nepoch 856/1000,loss=0.0001\n Accuracy= 1.0\nepoch 857/1000,loss=0.0001\n Accuracy= 1.0\nepoch 858/1000,loss=0.0000\n Accuracy= 1.0\nepoch 859/1000,loss=0.0000\n Accuracy= 1.0\nepoch 860/1000,loss=0.0001\n Accuracy= 1.0\nepoch 861/1000,loss=0.0000\n Accuracy= 1.0\nepoch 862/1000,loss=0.0001\n Accuracy= 1.0\nepoch 863/1000,loss=0.0001\n Accuracy= 1.0\nepoch 864/1000,loss=0.0001\n Accuracy= 1.0\nepoch 865/1000,loss=0.0000\n Accuracy= 1.0\nepoch 866/1000,loss=0.0001\n Accuracy= 1.0\nepoch 867/1000,loss=0.0001\n Accuracy= 1.0\nepoch 868/1000,loss=0.0000\n Accuracy= 1.0\nepoch 869/1000,loss=0.0001\n Accuracy= 1.0\nepoch 870/1000,loss=0.0001\n Accuracy= 1.0\nepoch 871/1000,loss=0.0001\n Accuracy= 1.0\nepoch 872/1000,loss=0.0001\n Accuracy= 1.0\nepoch 873/1000,loss=0.0000\n Accuracy= 1.0\nepoch 874/1000,loss=0.0001\n Accuracy= 1.0\nepoch 875/1000,loss=0.0001\n Accuracy= 1.0\nepoch 876/1000,loss=0.0001\n Accuracy= 1.0\nepoch 877/1000,loss=0.0001\n Accuracy= 1.0\nepoch 878/1000,loss=0.0000\n Accuracy= 1.0\nepoch 879/1000,loss=0.0001\n Accuracy= 1.0\nepoch 880/1000,loss=0.0000\n Accuracy= 1.0\nepoch 881/1000,loss=0.0001\n Accuracy= 1.0\nepoch 882/1000,loss=0.0001\n Accuracy= 1.0\nepoch 883/1000,loss=0.0001\n Accuracy= 1.0\nepoch 884/1000,loss=0.0001\n Accuracy= 1.0\nepoch 885/1000,loss=0.0001\n Accuracy= 1.0\nepoch 886/1000,loss=0.0000\n Accuracy= 1.0\nepoch 887/1000,loss=0.0001\n Accuracy= 1.0\nepoch 888/1000,loss=0.0001\n Accuracy= 1.0\nepoch 889/1000,loss=0.0001\n Accuracy= 1.0\nepoch 890/1000,loss=0.0000\n Accuracy= 1.0\nepoch 891/1000,loss=0.0001\n Accuracy= 1.0\nepoch 892/1000,loss=0.0000\n Accuracy= 1.0\nepoch 893/1000,loss=0.0001\n Accuracy= 1.0\nepoch 894/1000,loss=0.0001\n Accuracy= 1.0\nepoch 895/1000,loss=0.0000\n Accuracy= 1.0\nepoch 896/1000,loss=0.0001\n Accuracy= 1.0\nepoch 897/1000,loss=0.0001\n Accuracy= 1.0\nepoch 898/1000,loss=0.0001\n Accuracy= 1.0\nepoch 899/1000,loss=0.0001\n Accuracy= 1.0\nepoch 900/1000,loss=0.0000\n Accuracy= 1.0\nepoch 901/1000,loss=0.0000\n Accuracy= 1.0\nepoch 902/1000,loss=0.0001\n Accuracy= 1.0\nepoch 903/1000,loss=0.0001\n Accuracy= 1.0\nepoch 904/1000,loss=0.0001\n Accuracy= 1.0\nepoch 905/1000,loss=0.0000\n Accuracy= 1.0\nepoch 906/1000,loss=0.0001\n Accuracy= 1.0\nepoch 907/1000,loss=0.0000\n Accuracy= 1.0\nepoch 908/1000,loss=0.0001\n Accuracy= 1.0\nepoch 909/1000,loss=0.0000\n Accuracy= 1.0\nepoch 910/1000,loss=0.0001\n Accuracy= 1.0\nepoch 911/1000,loss=0.0001\n Accuracy= 1.0\nepoch 912/1000,loss=0.0001\n Accuracy= 1.0\nepoch 913/1000,loss=0.0000\n Accuracy= 1.0\nepoch 914/1000,loss=0.0000\n Accuracy= 1.0\nepoch 915/1000,loss=0.0001\n Accuracy= 1.0\nepoch 916/1000,loss=0.0001\n Accuracy= 1.0\nepoch 917/1000,loss=0.0001\n Accuracy= 1.0\nepoch 918/1000,loss=0.0000\n Accuracy= 1.0\nepoch 919/1000,loss=0.0001\n Accuracy= 1.0\nepoch 920/1000,loss=0.0000\n Accuracy= 1.0\nepoch 921/1000,loss=0.0001\n Accuracy= 1.0\nepoch 922/1000,loss=0.0001\n Accuracy= 1.0\nepoch 923/1000,loss=0.0000\n Accuracy= 1.0\nepoch 924/1000,loss=0.0000\n Accuracy= 1.0\nepoch 925/1000,loss=0.0001\n Accuracy= 1.0\nepoch 926/1000,loss=0.0001\n Accuracy= 1.0\nepoch 927/1000,loss=0.0001\n Accuracy= 1.0\nepoch 928/1000,loss=0.0001\n Accuracy= 1.0\nepoch 929/1000,loss=0.0001\n Accuracy= 1.0\nepoch 930/1000,loss=0.0001\n Accuracy= 1.0\nepoch 931/1000,loss=0.0001\n Accuracy= 1.0\nepoch 932/1000,loss=0.0000\n Accuracy= 1.0\nepoch 933/1000,loss=0.0001\n Accuracy= 1.0\nepoch 934/1000,loss=0.0001\n Accuracy= 1.0\nepoch 935/1000,loss=0.0000\n Accuracy= 1.0\nepoch 936/1000,loss=0.0001\n Accuracy= 1.0\nepoch 937/1000,loss=0.0000\n Accuracy= 1.0\nepoch 938/1000,loss=0.0000\n Accuracy= 1.0\nepoch 939/1000,loss=0.0001\n Accuracy= 1.0\nepoch 940/1000,loss=0.0001\n Accuracy= 1.0\nepoch 941/1000,loss=0.0000\n Accuracy= 1.0\nepoch 942/1000,loss=0.0001\n Accuracy= 1.0\nepoch 943/1000,loss=0.0001\n Accuracy= 1.0\nepoch 944/1000,loss=0.0001\n Accuracy= 1.0\nepoch 945/1000,loss=0.0001\n Accuracy= 1.0\nepoch 946/1000,loss=0.0001\n Accuracy= 1.0\nepoch 947/1000,loss=0.0001\n Accuracy= 1.0\nepoch 948/1000,loss=0.0000\n Accuracy= 1.0\nepoch 949/1000,loss=0.0001\n Accuracy= 1.0\nepoch 950/1000,loss=0.0001\n Accuracy= 1.0\nepoch 951/1000,loss=0.0001\n Accuracy= 1.0\nepoch 952/1000,loss=0.0000\n Accuracy= 1.0\nepoch 953/1000,loss=0.0000\n Accuracy= 1.0\nepoch 954/1000,loss=0.0000\n Accuracy= 1.0\nepoch 955/1000,loss=0.0000\n Accuracy= 1.0\nepoch 956/1000,loss=0.0000\n Accuracy= 1.0\nepoch 957/1000,loss=0.0001\n Accuracy= 1.0\nepoch 958/1000,loss=0.0001\n Accuracy= 1.0\nepoch 959/1000,loss=0.0001\n Accuracy= 1.0\nepoch 960/1000,loss=0.0001\n Accuracy= 1.0\nepoch 961/1000,loss=0.0001\n Accuracy= 1.0\nepoch 962/1000,loss=0.0000\n Accuracy= 1.0\nepoch 963/1000,loss=0.0000\n Accuracy= 1.0\nepoch 964/1000,loss=0.0000\n Accuracy= 1.0\nepoch 965/1000,loss=0.0000\n Accuracy= 1.0\nepoch 966/1000,loss=0.0000\n Accuracy= 1.0\nepoch 967/1000,loss=0.0001\n Accuracy= 1.0\nepoch 968/1000,loss=0.0000\n Accuracy= 1.0\nepoch 969/1000,loss=0.0000\n Accuracy= 1.0\nepoch 970/1000,loss=0.0000\n Accuracy= 1.0\nepoch 971/1000,loss=0.0001\n Accuracy= 1.0\nepoch 972/1000,loss=0.0001\n Accuracy= 1.0\nepoch 973/1000,loss=0.0001\n Accuracy= 1.0\nepoch 974/1000,loss=0.0000\n Accuracy= 1.0\nepoch 975/1000,loss=0.0000\n Accuracy= 1.0\nepoch 976/1000,loss=0.0000\n Accuracy= 1.0\nepoch 977/1000,loss=0.0001\n Accuracy= 1.0\nepoch 978/1000,loss=0.0001\n Accuracy= 1.0\nepoch 979/1000,loss=0.0001\n Accuracy= 1.0\nepoch 980/1000,loss=0.0001\n Accuracy= 1.0\nepoch 981/1000,loss=0.0000\n Accuracy= 1.0\nepoch 982/1000,loss=0.0000\n Accuracy= 1.0\nepoch 983/1000,loss=0.0001\n Accuracy= 1.0\nepoch 984/1000,loss=0.0000\n Accuracy= 1.0\nepoch 985/1000,loss=0.0001\n Accuracy= 1.0\nepoch 986/1000,loss=0.0000\n Accuracy= 1.0\nepoch 987/1000,loss=0.0001\n Accuracy= 1.0\nepoch 988/1000,loss=0.0001\n Accuracy= 1.0\nepoch 989/1000,loss=0.0001\n Accuracy= 1.0\nepoch 990/1000,loss=0.0001\n Accuracy= 1.0\nepoch 991/1000,loss=0.0000\n Accuracy= 1.0\nepoch 992/1000,loss=0.0001\n Accuracy= 1.0\nepoch 993/1000,loss=0.0001\n Accuracy= 1.0\nepoch 994/1000,loss=0.0001\n Accuracy= 1.0\nepoch 995/1000,loss=0.0001\n Accuracy= 1.0\nepoch 996/1000,loss=0.0001\n Accuracy= 1.0\nepoch 997/1000,loss=0.0000\n Accuracy= 1.0\nepoch 998/1000,loss=0.0000\n Accuracy= 1.0\nepoch 999/1000,loss=0.0001\n Accuracy= 1.0\nepoch 1000/1000,loss=0.0000\n Accuracy= 1.0\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OY2aTZXeRhL4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "2c839414-e5d9-445a-9c6a-b36f41d21ff9",
        "tags": []
      },
      "source": [
        "#now training complete \n",
        "#implement a chat environment\n",
        "my_model.eval()\n",
        "device=torch.device(\"cpu\")\n",
        "import random\n",
        "print('NaijaBot is ready to chat with you !!! / enter \"quit\" to leave the chatting ')\n",
        "print('lets start !')\n",
        "while True:\n",
        "    sentence = input('you :')\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "    #now tokenize ,stem and feed to network\n",
        "    tokenized=tokenize(sentence)\n",
        "    stemmed=[stemming(w) for w in tokenized]\n",
        "    my_vector=bag_of_words_converter(stemmed,all_words_array)\n",
        "    \n",
        "    my_vector=torch.from_numpy(my_vector)\n",
        "    my_vector=torch.unsqueeze(my_vector,0)\n",
        "    my_vector.to(device)\n",
        "    trained_model.to(device)\n",
        "    prediction=trained_model(my_vector)\n",
        "    prediction_probabilities=torch.softmax(prediction,dim=1)\n",
        "    predicted_tag_index=prediction.argmax(dim=1).item()\n",
        "#     print(predicted_tag_index)\n",
        "    actual_tag_predicted=tags[predicted_tag_index]\n",
        "#     print(actual_tag_predicted)\n",
        "    #also checking for probability so that it doesnot give unwanted answers\n",
        "    prob=prediction_probabilities[0,predicted_tag_index]\n",
        "#     print(prob)\n",
        "    if prob<0.7:\n",
        "        print(f\"_bot: Sorry, I cannot understand you...\" )\n",
        "        continue\n",
        "    \n",
        "    #now time for chatbot to give answer\n",
        "    for intent in intents[\"intents\"]:\n",
        "        if intent[\"tag\"]==actual_tag_predicted:\n",
        "            print(f\"_bot:\" + str(random.choice(intent[\"responses\"])))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "NaijaBot is ready to chat with you !!! / enter \"quit\" to leave the chatting \nlets start !\n_bot:AND TO PROVIDE for a Constitution for the purpose of promoting the good government and welfare of all persons in our country, on the principles of freedom, equality and justice, and for the purpose of consolidating the unity of our people\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:AND TO PROVIDE for a Constitution for the purpose of promoting the good government and welfare of all persons in our country, on the principles of freedom, equality and justice, and for the purpose of consolidating the unity of our people\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:AND TO PROVIDE for a Constitution for the purpose of promoting the good government and welfare of all persons in our country, on the principles of freedom, equality and justice, and for the purpose of consolidating the unity of our people\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:TO LIVE in unity and harmony as one indivisible and indissoluble sovereign nation under God, dedicated to the promotion of inter-African solidarity, world peace, international co-operation and understanding\n_bot:AND TO PROVIDE for a Constitution for the purpose of promoting the good government and welfare of all persons in our country, on the principles of freedom, equality and justice, and for the purpose of consolidating the unity of our people\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-520cbe4ac202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lets start !'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'you :'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-FZggKVTK10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}